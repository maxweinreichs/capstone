
=======================
ğŸ§  Prompt tÃ©cnico de implementaciÃ³n
=======================

ğŸ¯ OBJETIVO
Implementar un entorno de aprendizaje por refuerzo profundo (DRL) que permita a un agente aprender polÃ­ticas de pricing Ã³ptimas en un contexto de retail con mÃºltiples productos, tiendas y variaciones estacionales en la demanda.

---

ğŸ“š CONTEXTO MODELADO (extraÃ­do de 'Algoritmo_Capstone-FINAL.pdf')
- Se busca maximizar una utilidad esperada basada en una funciÃ³n de demanda dependiente del precio.
- La demanda del producto q en la tienda l durante el periodo t es:
  s_qlt âˆ¼ N(Î¼_qlt(p_qlt), Ïƒ^2_qlt)

  Donde:
  Î¼_qlt(p_qlt) = Î³_qlt Â· Ï_qlt Â· e^(âˆ’Î±_qlt Â· p_qlt)

- La varianza puede estimarse con:
  Ïƒ^2_qlt = Î¼_qlt Â· (1 + Î¼_qlt / Î¸_qlt)

- La utilidad depende de: ingresos por venta, costos de pedido, penalizaciÃ³n por faltantes y costos de inventario.
- En cada paso se resuelve un submodelo de optimizaciÃ³n para encontrar la mejor cantidad a pedir dado un conjunto de precios propuestos por el DRL.
- Para mÃ¡s detalles, revisar secciÃ³n 2 y 2.1 del PDF citado.

---

ğŸ§± ESTRUCTURA DEL ENTORNO

- Productos: 9
- Tiendas: 2
- Horizonte: 1 semana por ahora (stateless)
- Precio mÃ­nimo: p_qlt â‰¥ 1.05 Â· c_q (para garantizar al menos un 5% de margen)
- Precio mÃ¡ximo: libre (simulaciÃ³n propone tope como c_q + 50)

---

ğŸ”§ A IMPLEMENTAR

1. **SimulaciÃ³n de demanda**

Para cada combinaciÃ³n producto-tienda:

Î¼_ql = Î³_ql Â· Ï_ql Â· e^(âˆ’Î±_ql Â· p_ql)

donde:
- Î³_ql: estacionalidad (e.g., [1.2, 1.0, 0.8, 0.6])
- Ï_ql: demanda base (e.g., 100)
- Î±_ql: sensibilidad (e.g., 0.07)
- Î¸_ql: parÃ¡metro de dispersiÃ³n (e.g., 20)

Luego se genera:
s_ql âˆ¼ N(Î¼_ql, Ïƒ^2_ql)

2. **Submodelo de optimizaciÃ³n usando Gurobi**

Para cada producto q, tienda l:

Maximizar:
    min(s_ql, I_ql + o_ql) * p_ql 
    - c_q * o_ql 
    - K_q * y_ql 
    - h_q * I_ql 
    - d_ql * max(0, s_ql - I_ql - o_ql)

Sujeto a:
- o_ql â‰¥ 0
- I_ql + o_ql â‰¤ IF_l
- o_ql â‰¤ M * y_ql
- Variables auxiliares: y_ql binaria (1 si hay pedido), I_ql inventario inicial (fijar en 30)

ParÃ¡metros recomendados:
- c_q: dado
- h_q: 0.5
- d_ql: 2.0
- IF_l: 100
- K_q: 5

3. **IntegraciÃ³n en entorno Gym**

- AcciÃ³n: vector p_ql (precios propuestos por el agente)
- Reward: utilidad obtenida resolviendo el submodelo anterior con demanda simulada
- ObservaciÃ³n: dummy (stateless)
- Reset: reinicia el entorno
- Step:
    - Recibe precios
    - Simula demanda
    - Resuelve optimizaciÃ³n
    - Devuelve reward

4. **Entrenamiento DRL**

```python
from stable_baselines3 import PPO

env = PrecioStatelessEnv()
model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)
```

---

ğŸ“ ARCHIVOS ESPERADOS

- entorno_precio.py â†’ contiene clase PrecioStatelessEnv
- demanda.py â†’ contiene funciÃ³n simular_demanda(p)
- optimizador.py â†’ contiene funciÃ³n resolver_subproblema(p, s)
- main.py â†’ entrena el modelo y testea resultados

---

ğŸ’¡ NOTAS FINALES
- Este prompt estÃ¡ diseÃ±ado para iniciar implementaciÃ³n directa, sin redefinir el problema.
- Basado en el documento tÃ©cnico â€˜Algoritmo_Capstone-FINAL.pdfâ€™, que incluye funciÃ³n de demanda, funciÃ³n objetivo y restricciones exactas del modelo formal.

